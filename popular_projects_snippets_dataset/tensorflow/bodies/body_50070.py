# Extracted from ./data/repos/tensorflow/tensorflow/python/keras/optimizer_v2/adam.py
params = self.weights
# If the weights are generated by Keras V1 optimizer, it includes vhats
# even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2
# optimizer has 2x + 1 variables. Filter vhats out for compatibility.
num_vars = int((len(params) - 1) / 2)
if len(weights) == 3 * num_vars + 1:
    weights = weights[:len(params)]
super(Adam, self).set_weights(weights)
