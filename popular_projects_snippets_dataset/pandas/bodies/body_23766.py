# Extracted from ./data/repos/pandas/pandas/io/pytables.py
"""
    Read from the store, close it if we opened it.

    Retrieve pandas object stored in file, optionally based on where
    criteria.

    .. warning::

       Pandas uses PyTables for reading and writing HDF5 files, which allows
       serializing object-dtype data with pickle when using the "fixed" format.
       Loading pickled data received from untrusted sources can be unsafe.

       See: https://docs.python.org/3/library/pickle.html for more.

    Parameters
    ----------
    path_or_buf : str, path object, pandas.HDFStore
        Any valid string path is acceptable. Only supports the local file system,
        remote URLs and file-like objects are not supported.

        If you want to pass in a path object, pandas accepts any
        ``os.PathLike``.

        Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.

    key : object, optional
        The group identifier in the store. Can be omitted if the HDF file
        contains a single pandas object.
    mode : {'r', 'r+', 'a'}, default 'r'
        Mode to use when opening the file. Ignored if path_or_buf is a
        :class:`pandas.HDFStore`. Default is 'r'.
    errors : str, default 'strict'
        Specifies how encoding and decoding errors are to be handled.
        See the errors argument for :func:`open` for a full list
        of options.
    where : list, optional
        A list of Term (or convertible) objects.
    start : int, optional
        Row number to start selection.
    stop  : int, optional
        Row number to stop selection.
    columns : list, optional
        A list of columns names to return.
    iterator : bool, optional
        Return an iterator object.
    chunksize : int, optional
        Number of rows to include in an iteration when using an iterator.
    **kwargs
        Additional keyword arguments passed to HDFStore.

    Returns
    -------
    object
        The selected object. Return type depends on the object stored.

    See Also
    --------
    DataFrame.to_hdf : Write a HDF file from a DataFrame.
    HDFStore : Low-level access to HDF files.

    Examples
    --------
    >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP
    >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP
    >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP
    """
if mode not in ["r", "r+", "a"]:
    raise ValueError(
        f"mode {mode} is not allowed while performing a read. "
        f"Allowed modes are r, r+ and a."
    )
# grab the scope
if where is not None:
    where = _ensure_term(where, scope_level=1)

if isinstance(path_or_buf, HDFStore):
    if not path_or_buf.is_open:
        raise OSError("The HDFStore must be open for reading.")

    store = path_or_buf
    auto_close = False
else:
    path_or_buf = stringify_path(path_or_buf)
    if not isinstance(path_or_buf, str):
        raise NotImplementedError(
            "Support for generic buffers has not been implemented."
        )
    try:
        exists = os.path.exists(path_or_buf)

    # if filepath is too long
    except (TypeError, ValueError):
        exists = False

    if not exists:
        raise FileNotFoundError(f"File {path_or_buf} does not exist")

    store = HDFStore(path_or_buf, mode=mode, errors=errors, **kwargs)
    # can't auto open/close if we are using an iterator
    # so delegate to the iterator
    auto_close = True

try:
    if key is None:
        groups = store.groups()
        if len(groups) == 0:
            raise ValueError(
                "Dataset(s) incompatible with Pandas data types, "
                "not table, or no datasets found in HDF5 file."
            )
        candidate_only_group = groups[0]

        # For the HDF file to have only one dataset, all other groups
        # should then be metadata groups for that candidate group. (This
        # assumes that the groups() method enumerates parent groups
        # before their children.)
        for group_to_check in groups[1:]:
            if not _is_metadata_of(group_to_check, candidate_only_group):
                raise ValueError(
                    "key must be provided when HDF5 "
                    "file contains multiple datasets."
                )
        key = candidate_only_group._v_pathname
    exit(store.select(
        key,
        where=where,
        start=start,
        stop=stop,
        columns=columns,
        iterator=iterator,
        chunksize=chunksize,
        auto_close=auto_close,
    ))
except (ValueError, TypeError, KeyError):
    if not isinstance(path_or_buf, HDFStore):
        # if there is an error, close the store if we opened it.
        with suppress(AttributeError):
            store.close()

    raise
