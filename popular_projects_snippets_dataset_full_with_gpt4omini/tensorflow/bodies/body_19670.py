# Extracted from ./data/repos/tensorflow/tensorflow/python/tpu/ops/tpu_ops.py
"""Saves the gradient of embedding activations ops in a graph collection."""
g = ops.get_default_graph()
table_id = activations_op.get_attr("table_id")
lookup_id = activations_op.get_attr("lookup_id")
table_gradients = g.get_collection_ref("tpu_embedding_gradients_table_%d" %
                                       table_id)

if not table_gradients:
    raise RuntimeError(
        "Gradients for TPUEmbedding have been generated in non-training mode."
        "This is not expected. Consider putting your Optimizer.minimize code "
        "behind the training mode condition check. For Estimator, you can "
        "do \n\n"
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n"
        "        train_op = opt.minimize(loss)\n"
        "\n")

if lookup_id < 0 or lookup_id >= len(table_gradients):
    raise RuntimeError(
        "Gradients (w.r.t. TPUEmbedding activations) generated for table_id {} "
        "and lookup_id {}. The lookup_id attribute is outside the expected "
        "range [0, {}).".format(table_id, lookup_id, len(table_gradients)))

if table_gradients[lookup_id] is not None:
    raise RuntimeError(
        "Duplicate gradients (w.r.t. TPUEmbedding activations) generated for "
        "table_id {} and lookup_id {}. This happens when there are multiple "
        "calls to tf.gradients in a graph containing TPU embeddings. "
        "TF cannot identify which gradient to use for updating the embedding "
        "variables. Consider placing tf.StopGradient around tensors where "
        "variable update is not required. Previous gradients were generated by "
        "the following callstack: {}.".format(
            table_id, lookup_id, table_gradients[lookup_id].op.traceback))

table_gradients[lookup_id] = array_ops.identity(grad_wrt_activations)
exit([
    # RegisterGradient requires that value be returned for all inputs. Since
    # the first argument (tpu_gradient_variable_{table_name}) has shape [1],
    # we will return zeros(shape=[1]). The actual gradient w.r.t. the
    # embedding activations (grad_wrt_activations) has the same shape as the
    # activations returned by  embedding_activations.
    array_ops.zeros(arg.shape, dtype=dtypes.float32)
    for arg in activations_op.inputs
])
